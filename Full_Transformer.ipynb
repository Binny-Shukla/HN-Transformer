{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4b3ff1",
   "metadata": {},
   "source": [
    "# Topic 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e59d0",
   "metadata": {},
   "source": [
    "**Positional Encoding** This is used as in LLM they do not have recurrence as in sequential models they do not know how to treat sequence so for getting the position of tokens and providing sequence.\n",
    "\n",
    "Positional encoding is used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc6736b",
   "metadata": {},
   "source": [
    "## *Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f80a9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import json\n",
    "import math\n",
    "import sentencepiece as spm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e09396",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings( category = FutureWarning , action =  'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc9c2a",
   "metadata": {},
   "source": [
    "##### Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccecaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "print(f'Device : {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502ec41",
   "metadata": {},
   "source": [
    "## *Pos Encoding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d784401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positonal_encoding(seq_length , d_model):\n",
    "    \n",
    "    pe = torch.zeros(seq_length , d_model)\n",
    "    postions = torch.arange(0 , seq_length , dtype = torch.float32).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0 , d_model , 2).float() * (-math.log(10_000) / d_model))\n",
    "    \n",
    "    pe[: , 0::2] = torch.sin(postions * div_term)\n",
    "    pe[: , 1::2] = torch.cos(postions * div_term)\n",
    "    \n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda1891",
   "metadata": {},
   "source": [
    "# Attention Mechanism "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d91dc",
   "metadata": {},
   "source": [
    "The core of transformer is **Attention** we will start with *scaled dot product attention* \n",
    "\n",
    "1. In this a position token give attention to other tokens\n",
    "\n",
    "2. It helps to weighs the importance in the embedding of each token\n",
    "\n",
    "3. Softmax converts these weights to probabilites\n",
    "\n",
    "4. The dot product helps to see similarity between the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164b37bb",
   "metadata": {},
   "source": [
    "## *Scaled Dot product Attention*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d4b3be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " tensor([[[0.3321, 0.1747, 0.3468, 0.5297, 0.3732, 0.3436, 0.6892, 0.5414],\n",
      "         [0.3129, 0.1666, 0.3750, 0.5380, 0.3835, 0.4080, 0.6862, 0.4949],\n",
      "         [0.3150, 0.1728, 0.3638, 0.5439, 0.3841, 0.3909, 0.6881, 0.5145],\n",
      "         [0.3086, 0.1618, 0.3800, 0.5294, 0.3872, 0.4023, 0.6989, 0.5016]]])\n",
      "Attention Weights:\n",
      " tensor([[[0.2528, 0.2606, 0.2357, 0.2510],\n",
      "         [0.2149, 0.2610, 0.1979, 0.3262],\n",
      "         [0.2088, 0.2596, 0.2266, 0.3050],\n",
      "         [0.2228, 0.2409, 0.2122, 0.3241]]])\n"
     ]
    }
   ],
   "source": [
    "def scaled_dot_product_attention(Q , K , V):\n",
    "    \n",
    "    # Query is a 3D (batch , seq_length , d_k)\n",
    "    \n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Matmul is matrix multiplication\n",
    "    \n",
    "    scores = torch.matmul(Q , K.transpose(-2 , -1) / torch.sqrt(torch.tensor(d_k , dtype = torch.float32)))\n",
    "    \n",
    "    # K also has same dimension but the transpose changes its dimension to (batch , d_k , seq)\n",
    "    \n",
    "    attn_weights = torch.softmax(scores , dim = -1)\n",
    "    \n",
    "    # softmax convert the weights to probabilities\n",
    "    \n",
    "    output = torch.matmul(attn_weights , V)\n",
    "    \n",
    "    return output , attn_weights\n",
    "\n",
    "\n",
    "\n",
    "# Example input (batch size = 1, seq_len = 4, d_k = 8)\n",
    "Q = torch.rand(1, 4, 8)\n",
    "K = torch.rand(1, 4, 8)\n",
    "V = torch.rand(1, 4, 8)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(\"Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attn_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86fca4",
   "metadata": {},
   "source": [
    "# *Multi Head Attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8025bec",
   "metadata": {},
   "source": [
    "In this technique parallel layers will focus attention on dimension and is far more significantly impactful then single dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4911049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(Multi_Head_Attention , self).__init__()\n",
    "    \n",
    "        # Assert if num heads are applicable\n",
    "        try :\n",
    "            assert d_model % num_heads == 0\n",
    "        except:\n",
    "            print(f'Number of heads are not applicable on the dimension of model')\n",
    "            \n",
    "            \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        \n",
    "        # Create linear layers for Q , K , V\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        \n",
    "        self.projection = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def single_dot_attention(self, Q, K, V):\n",
    "        \n",
    "        d_k = Q.size(-1)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype = torch.float32))\n",
    "        \n",
    "        attention_weights = torch.softmax(scores, dim = -1)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projection\n",
    "        \n",
    "        Q = self.W_Q(query)\n",
    "        K = self.W_K(key)\n",
    "        V = self.W_V(value)\n",
    "        \n",
    "        def reshape(x):\n",
    "            x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            \n",
    "            return x\n",
    "\n",
    "        Q = reshape(Q)\n",
    "        K = reshape(K)\n",
    "        V = reshape(V)\n",
    "        \n",
    "        # Now add attention\n",
    "        \n",
    "        output , attn = self.single_dot_attention(Q, K, V)\n",
    "        \n",
    "        output = output.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        projection_out = self.projection(output)\n",
    "        \n",
    "        return projection_out , attn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ba87dc",
   "metadata": {},
   "source": [
    "# *Encoded Layer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af4c18",
   "metadata": {},
   "source": [
    "It intact all the layers:\n",
    "\n",
    "1.  Optinal(Layer Normalization)\n",
    "\n",
    "1. MHA\n",
    "\n",
    "2. Layer Normalization\n",
    "\n",
    "3. Feed forward Network\n",
    "\n",
    "4. Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d6cd1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        super(Transformer_Encoder, self).__init__()\n",
    "        \n",
    "        # Normalization\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        \n",
    "        # MHA\n",
    "        \n",
    "        self.MHA = Multi_Head_Attention(d_model, num_heads)\n",
    "        \n",
    "        # Layer Normalization 2\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Feed forward Network\n",
    "        \n",
    "        self.FFN = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_ff, d_ff),\n",
    "            \n",
    "            \n",
    "            nn.Linear(d_ff, d_ff),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer Normalization 3\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout layer\n",
    "        \n",
    "        self.drop_1 = nn.Dropout(dropout)\n",
    "        self.drop_2 = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Pass to norm 1\n",
    "        \n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Pass to MHA\n",
    "        \n",
    "        attn_output, _ = self.MHA(x, x, x)\n",
    "        \n",
    "        # Pass to norm 2\n",
    "        \n",
    "        x = self.norm2(x + self.drop_1(attn_output))\n",
    "        \n",
    "        # Pass to FFN\n",
    "        \n",
    "        ffn = self.FFN(x)\n",
    "        \n",
    "        # Pass to norm 3\n",
    "        \n",
    "        x = self.norm3(x + self.drop_2(ffn))\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e9d3e",
   "metadata": {},
   "source": [
    "# *Stack*\n",
    "\n",
    "Here we will stack the encoded layer for parallel and significant processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34c606fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Stack(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, num_heads, num_layer):\n",
    "        super(Transformer_Stack, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.layer = nn.ModuleList([Transformer_Encoder(d_model, num_heads, d_ff)\n",
    "                                    for _ in range(num_layer)])\n",
    "        \n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        pe = positonal_encoding(seq_length, self.d_model).unsqueeze(0).to(x.device)\n",
    "        \n",
    "        x = x + pe\n",
    "        \n",
    "        for layer in self.layer:\n",
    "            x = layer(x)\n",
    "            \n",
    "            \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c56db8",
   "metadata": {},
   "source": [
    "### Initailizating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28845d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder_stack = Transformer_Stack(\n",
    "    num_layer=6,                        \n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048\n",
    ")\n",
    "\n",
    "dummy_input = torch.rand(32, 50, 512)  # (batch_size, seq_len, d_model)\n",
    "out = encoder_stack(dummy_input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951b176",
   "metadata": {},
   "source": [
    "# *Transformer Decoder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "453f6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Decoder(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_heads, dropout = 0.1):\n",
    "        super(Transformer_Decoder, self).__init__()\n",
    "        \n",
    "        # Masked Self attention \n",
    "        \n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.masked = Multi_Head_Attention(d_model, num_heads)\n",
    "        \n",
    "        # Encoder Decoder Attention\n",
    "        \n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        self.enc_dec = Multi_Head_Attention(d_model, num_heads)\n",
    "        \n",
    "        # Feed Forward Network\n",
    "        \n",
    "        self.norm_3 = nn.LayerNorm(d_model)\n",
    "        self.FFN = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(d_ff, d_ff),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Norm\n",
    "        \n",
    "        self.norm_4 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropouts\n",
    "        \n",
    "        self.drop_1 = nn.Dropout(dropout)\n",
    "        self.drop_2 = nn.Dropout(dropout)\n",
    "        self.drop_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, enc_out, mask = None):\n",
    "        \n",
    "        # LayerNorm -> Masked \n",
    "        \n",
    "        x_2 = self.norm_1(x)\n",
    "        \n",
    "        attn_output, _ = self.masked(x_2, x_2, x_2 , mask)\n",
    "        \n",
    "        # Masked -> Drop -> LayerNorm -> Enc-Dec\n",
    "        \n",
    "        x = x + self.drop_1(attn_output)\n",
    "        \n",
    "        x_2 = self.norm_2(x)\n",
    "        \n",
    "        attn_output_2, _ = self.enc_dec(x_2, enc_out, enc_out)\n",
    "        \n",
    "        # Enc-Dec -> Drop -> LayerNorm -> FFN\n",
    "        \n",
    "        x = x + self.drop_2(attn_output_2)\n",
    "        \n",
    "        x_2 = self.norm_3(x)\n",
    "        \n",
    "        ffn_out = self.FFN(x_2)\n",
    "        \n",
    "        x_2 = x + self.drop_3(ffn_out)\n",
    "        \n",
    "        x = self.norm_4(x_2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5bb3ef",
   "metadata": {},
   "source": [
    "# *Masking*\n",
    "\n",
    "It prevents the transformer to see the future tokens and help it to learn to adjust what is has produced till now\n",
    "without it the transformer will never learn thus large losses and increased training time can be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79106f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(seq_length):\n",
    "    \n",
    "    mask = torch.triu(torch.ones(seq_length, seq_length) , diagonal = 1)\n",
    "    \n",
    "    mask = mask.masked_fill(mask == 1 , -torch.inf)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f336e",
   "metadata": {},
   "source": [
    "Stacking the Decoding Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506bd18",
   "metadata": {},
   "source": [
    "# *Encoder Decoder Stack*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c096aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Decoder_Stack(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, num_heads, num_layer):\n",
    "        super(Encoder_Decoder_Stack, self).__init__()\n",
    "        \n",
    "        self.layer = nn.ModuleList([Transformer_Decoder(d_model, d_ff, num_heads)\n",
    "                                    for _ in range(num_layer)])\n",
    "        \n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x, enc_out):\n",
    "        \n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        pe = positonal_encoding(seq_length, self.d_model).to(x.device)\n",
    "        \n",
    "        x = x + pe\n",
    "        \n",
    "        mask = generate_mask(seq_length).to(x.device)\n",
    "        \n",
    "        for layer in self.layer:\n",
    "            x = layer(x, enc_out, mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b1e7c",
   "metadata": {},
   "source": [
    "### *Initialize*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c694a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "decoder_stack = Encoder_Decoder_Stack(\n",
    "    num_layer=6,     \n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048\n",
    ")\n",
    "\n",
    "dummy_input = torch.rand(32, 50, 512)  # (batch_size, seq_len, d_model)\n",
    "out = encoder_stack(dummy_input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56a2e4",
   "metadata": {},
   "source": [
    "# *Full Transformer*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23f46db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_ff, d_model, num_heads, num_layers, dropout = 0.1):\n",
    "        super(Full_Transformer, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings\n",
    "        \n",
    "        self.src_embedd = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedd = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Encoder Decoder Stack\n",
    "        \n",
    "        self.encoder = Transformer_Stack(d_model, d_ff, num_heads, num_layers)\n",
    "        self.decoder = Encoder_Decoder_Stack(d_model, d_ff, num_heads, num_layers)\n",
    "        \n",
    "        # Final Projection\n",
    "        \n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Dropout\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        \n",
    "        src_seq_length = src.size(1)\n",
    "        tgt_seq_length = tgt.size(1)\n",
    "        \n",
    "        # Embeddings\n",
    "        \n",
    "        src = self.src_embedd(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.tgt_embedd(tgt) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # Pos Encoding\n",
    "        \n",
    "        src_pe = positonal_encoding(src_seq_length, self.d_model).unsqueeze(0).to(src.device)\n",
    "        tgt_pe = positonal_encoding(tgt_seq_length, self.d_model).unsqueeze(0).to(tgt.device)\n",
    "        \n",
    "        # Drops\n",
    "        \n",
    "        src = self.drop(src + src_pe)\n",
    "        tgt = self.drop(tgt + tgt_pe)\n",
    "        \n",
    "        # Encoder pass\n",
    "        \n",
    "        enc_out = self.encoder(src)\n",
    "        \n",
    "        # Decoder pass\n",
    "        \n",
    "        dec_out = self.decoder(tgt, enc_out)\n",
    "        \n",
    "        # Final Projection\n",
    "        \n",
    "        out = self.projection(dec_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6707341",
   "metadata": {},
   "source": [
    "# *Tokenizer*\n",
    "\n",
    "Transformers works on numeric ids then words which leads to need of tokenizer.\n",
    "The **Tokenizer** used in this transformer is **Sentence Piece** and not an scratch cause it was not possible for me to train such large dataset on a manual build tokenizer i would eventually run of **memory and compututaion**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61ef9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input = 'C:\\LLM & Agents\\HN.json',\n",
    "    model_prefix = 'HN_Tokenizer',\n",
    "    vocab_size = 800\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "519aa744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 235, 505, 58, 90, 28, 142, 114, 219, 4, 18, 4, 15, 288, 219, 4, 743, 89, 744, 143, 12, 4, 59, 26, 78, 54]\n",
      "What is the first law and most important principle\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "\n",
    "# Loading the tokenizer\n",
    "sp.Load('HN_Tokenizer.model')\n",
    "\n",
    "tokens = sp.Encode('What is the first law and most important principle' , int)\n",
    "print(tokens)\n",
    "\n",
    "decode = sp.decode(tokens)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f943fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = tgt_vocab_size = 800\n",
    "\n",
    "Champion = Full_Transformer(\n",
    "    src_vocab_size,\n",
    "    tgt_vocab_size,\n",
    "    d_ff = 2048,\n",
    "    d_model = 512,\n",
    "    num_heads = 8,\n",
    "    num_layers = 6\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c8a06",
   "metadata": {},
   "source": [
    "\n",
    "# *DataSet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c868b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hn_dataset(json_path):\n",
    "    \n",
    "    data_pairs = []\n",
    "    \n",
    "    with open(json_path, 'r', encoding = 'utf-8') as F:\n",
    "        \n",
    "        data = json.load(F)\n",
    "        \n",
    "        \n",
    "    for entry in data['laws_of_human_nature']:\n",
    "        \n",
    "        src_txt = f\"{entry['law_number']} {entry['law_name']} {entry['key_principle']} {' '.join(entry['examples'])}\"\n",
    "        tgt_txt = f\"{entry['summary']} {' '.join(entry['application_tips'])} {' '.join(entry['cautions'])}\"\n",
    "        \n",
    "        data_pairs.append((src_txt, tgt_txt))\n",
    "        \n",
    "    return data_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "638bcb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_function(data_pairs, sp_model):\n",
    "    \n",
    "    tokenized_pair = []\n",
    "    \n",
    "    for src_txt , tgt_txt in data_pairs:\n",
    "        \n",
    "        src_ids = sp_model.Encode(src_txt, int)\n",
    "        tgt_ids = sp_model.encode(tgt_txt, int)\n",
    "        \n",
    "        tokenized_pair.append((src_ids, tgt_ids))\n",
    "        \n",
    "    return tokenized_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3671d971",
   "metadata": {},
   "source": [
    "#### Building Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e89b24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HN_PyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenized_pairs, pdx_idx, max_len):\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.data = tokenized_pairs\n",
    "        self.pdx_idx = pdx_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        src_ids, tgt_ids = self.data[index]\n",
    "        \n",
    "        # Pad to max len\n",
    "        \n",
    "        src_ids = src_ids[:self.max_len] + [self.pdx_idx] * (self.max_len - len(src_ids))\n",
    "        tgt_ids = tgt_ids[:self.max_len] + [self.pdx_idx] * (self.max_len - len(tgt_ids))\n",
    "        \n",
    "        # Teacher forcing\n",
    "        \n",
    "        tgt_input = tgt_ids[:-1]\n",
    "        tgt_output = tgt_ids[1:]\n",
    "        \n",
    "        return torch.tensor(src_ids), torch.tensor(tgt_input), torch.tensor(tgt_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc4902",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "663d91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data Pairs\n",
    "\n",
    "data_pairs = hn_dataset('C:\\LLM & Agents\\HN.json')\n",
    "\n",
    "# Feed to get tokenized Pairs\n",
    "\n",
    "tokenized_pair = tokenized_function(data_pairs, sp)\n",
    "\n",
    "# Create Pytorch Dataset\n",
    "\n",
    "hn_data = HN_PyDataset(tokenized_pair, pdx_idx = 0, max_len = 128)\n",
    "\n",
    "# Load Data\n",
    "\n",
    "Hn_loader = DataLoader(hn_data, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4c0f2",
   "metadata": {},
   "source": [
    "# **Loss and Optimizers**\n",
    "\n",
    "Optimizer used here is **AdamW** which is an optimial choice over the Adam which will provide with gradual decay\n",
    "\n",
    "Loss used here is **CrossEntropyLoss** as it is similar to multi classficiation task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca53e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "optimizer = optim.AdamW(Champion.parameters(), lr = 1e-4)\n",
    "\n",
    "# Loss\n",
    "\n",
    "Loss_Func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Scheduler\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 10, gamma = 0.997)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48afb917",
   "metadata": {},
   "source": [
    "# **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38f6b865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | AVG Loss: 6.7652| Loss: 6.7652\n",
      "Epoch 2 | AVG Loss: 6.5030| Loss: 6.5030\n",
      "Epoch 3 | AVG Loss: 6.2570| Loss: 6.2570\n",
      "Epoch 4 | AVG Loss: 5.9890| Loss: 5.9890\n",
      "Epoch 5 | AVG Loss: 5.7596| Loss: 5.7596\n",
      "Epoch 6 | AVG Loss: 5.5537| Loss: 5.5537\n",
      "Epoch 7 | AVG Loss: 5.3948| Loss: 5.3948\n",
      "Epoch 8 | AVG Loss: 5.2456| Loss: 5.2456\n",
      "Epoch 9 | AVG Loss: 5.1086| Loss: 5.1086\n",
      "Epoch 10 | AVG Loss: 4.9738| Loss: 4.9738\n",
      "Epoch 11 | AVG Loss: 4.8759| Loss: 4.8759\n",
      "Epoch 12 | AVG Loss: 4.7838| Loss: 4.7838\n",
      "Epoch 13 | AVG Loss: 4.6968| Loss: 4.6968\n",
      "Epoch 14 | AVG Loss: 4.6156| Loss: 4.6156\n",
      "Epoch 15 | AVG Loss: 4.5329| Loss: 4.5329\n",
      "Epoch 16 | AVG Loss: 4.4535| Loss: 4.4535\n",
      "Epoch 17 | AVG Loss: 4.3758| Loss: 4.3758\n",
      "Epoch 18 | AVG Loss: 4.3001| Loss: 4.3001\n",
      "Epoch 19 | AVG Loss: 4.2290| Loss: 4.2290\n",
      "Epoch 20 | AVG Loss: 4.1511| Loss: 4.1511\n",
      "Epoch 21 | AVG Loss: 4.0812| Loss: 4.0812\n",
      "Epoch 22 | AVG Loss: 4.0022| Loss: 4.0022\n",
      "Epoch 23 | AVG Loss: 3.9242| Loss: 3.9242\n",
      "Epoch 24 | AVG Loss: 3.8494| Loss: 3.8494\n",
      "Epoch 25 | AVG Loss: 3.7746| Loss: 3.7746\n",
      "Epoch 26 | AVG Loss: 3.7017| Loss: 3.7017\n",
      "Epoch 27 | AVG Loss: 3.6223| Loss: 3.6223\n",
      "Epoch 28 | AVG Loss: 3.5460| Loss: 3.5460\n",
      "Epoch 29 | AVG Loss: 3.4704| Loss: 3.4704\n",
      "Epoch 30 | AVG Loss: 3.3868| Loss: 3.3868\n",
      "Epoch 31 | AVG Loss: 3.3010| Loss: 3.3010\n",
      "Epoch 32 | AVG Loss: 3.2174| Loss: 3.2174\n",
      "Epoch 33 | AVG Loss: 3.1289| Loss: 3.1289\n",
      "Epoch 34 | AVG Loss: 3.0453| Loss: 3.0453\n",
      "Epoch 35 | AVG Loss: 2.9501| Loss: 2.9501\n",
      "Epoch 36 | AVG Loss: 2.8658| Loss: 2.8658\n",
      "Epoch 37 | AVG Loss: 2.7743| Loss: 2.7743\n",
      "Epoch 38 | AVG Loss: 2.6824| Loss: 2.6824\n",
      "Epoch 39 | AVG Loss: 2.5933| Loss: 2.5933\n",
      "Epoch 40 | AVG Loss: 2.5032| Loss: 2.5032\n",
      "Epoch 41 | AVG Loss: 2.4130| Loss: 2.4130\n",
      "Epoch 42 | AVG Loss: 2.3308| Loss: 2.3308\n",
      "Epoch 43 | AVG Loss: 2.2435| Loss: 2.2435\n",
      "Epoch 44 | AVG Loss: 2.1632| Loss: 2.1632\n",
      "Epoch 45 | AVG Loss: 2.0870| Loss: 2.0870\n",
      "Epoch 46 | AVG Loss: 1.9962| Loss: 1.9962\n",
      "Epoch 47 | AVG Loss: 1.9247| Loss: 1.9247\n",
      "Epoch 48 | AVG Loss: 1.8452| Loss: 1.8452\n",
      "Epoch 49 | AVG Loss: 1.7643| Loss: 1.7643\n",
      "Epoch 50 | AVG Loss: 1.6951| Loss: 1.6951\n",
      "Epoch 51 | AVG Loss: 1.6180| Loss: 1.6180\n",
      "Epoch 52 | AVG Loss: 1.5467| Loss: 1.5467\n",
      "Epoch 53 | AVG Loss: 1.4739| Loss: 1.4739\n",
      "Epoch 54 | AVG Loss: 1.4106| Loss: 1.4106\n",
      "Epoch 55 | AVG Loss: 1.3406| Loss: 1.3406\n",
      "Epoch 56 | AVG Loss: 1.2744| Loss: 1.2744\n",
      "Epoch 57 | AVG Loss: 1.2103| Loss: 1.2103\n",
      "Epoch 58 | AVG Loss: 1.1492| Loss: 1.1492\n",
      "Epoch 59 | AVG Loss: 1.0914| Loss: 1.0914\n",
      "Epoch 60 | AVG Loss: 1.0311| Loss: 1.0311\n",
      "Epoch 61 | AVG Loss: 0.9781| Loss: 0.9781\n",
      "Epoch 62 | AVG Loss: 0.9190| Loss: 0.9190\n",
      "Epoch 63 | AVG Loss: 0.8752| Loss: 0.8752\n",
      "Epoch 64 | AVG Loss: 0.8314| Loss: 0.8314\n",
      "Epoch 65 | AVG Loss: 0.7867| Loss: 0.7867\n",
      "Epoch 66 | AVG Loss: 0.7427| Loss: 0.7427\n",
      "Epoch 67 | AVG Loss: 0.7009| Loss: 0.7009\n",
      "Epoch 68 | AVG Loss: 0.6554| Loss: 0.6554\n",
      "Epoch 69 | AVG Loss: 0.6227| Loss: 0.6227\n",
      "Epoch 70 | AVG Loss: 0.5848| Loss: 0.5848\n",
      "Epoch 71 | AVG Loss: 0.5543| Loss: 0.5543\n",
      "Epoch 72 | AVG Loss: 0.5209| Loss: 0.5209\n",
      "Epoch 73 | AVG Loss: 0.4889| Loss: 0.4889\n",
      "Epoch 74 | AVG Loss: 0.4621| Loss: 0.4621\n",
      "Epoch 75 | AVG Loss: 0.4350| Loss: 0.4350\n",
      "Epoch 76 | AVG Loss: 0.4089| Loss: 0.4089\n",
      "Epoch 77 | AVG Loss: 0.3844| Loss: 0.3844\n",
      "Epoch 78 | AVG Loss: 0.3647| Loss: 0.3647\n",
      "Epoch 79 | AVG Loss: 0.3428| Loss: 0.3428\n",
      "Epoch 80 | AVG Loss: 0.3254| Loss: 0.3254\n",
      "Epoch 81 | AVG Loss: 0.3045| Loss: 0.3045\n",
      "Epoch 82 | AVG Loss: 0.2888| Loss: 0.2888\n",
      "Epoch 83 | AVG Loss: 0.2720| Loss: 0.2720\n",
      "Epoch 84 | AVG Loss: 0.2589| Loss: 0.2589\n",
      "Epoch 85 | AVG Loss: 0.2434| Loss: 0.2434\n",
      "Epoch 86 | AVG Loss: 0.2309| Loss: 0.2309\n",
      "Epoch 87 | AVG Loss: 0.2185| Loss: 0.2185\n",
      "Epoch 88 | AVG Loss: 0.2081| Loss: 0.2081\n",
      "Epoch 89 | AVG Loss: 0.1978| Loss: 0.1978\n",
      "Epoch 90 | AVG Loss: 0.1864| Loss: 0.1864\n",
      "Epoch 91 | AVG Loss: 0.1788| Loss: 0.1788\n",
      "Epoch 92 | AVG Loss: 0.1696| Loss: 0.1696\n",
      "Epoch 93 | AVG Loss: 0.1627| Loss: 0.1627\n",
      "Epoch 94 | AVG Loss: 0.1558| Loss: 0.1558\n",
      "Epoch 95 | AVG Loss: 0.1483| Loss: 0.1483\n",
      "Epoch 96 | AVG Loss: 0.1427| Loss: 0.1427\n",
      "Epoch 97 | AVG Loss: 0.1371| Loss: 0.1371\n",
      "Epoch 98 | AVG Loss: 0.1306| Loss: 0.1306\n",
      "Epoch 99 | AVG Loss: 0.1270| Loss: 0.1270\n",
      "Epoch 100 | AVG Loss: 0.1206| Loss: 0.1206\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    Champion.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    #with torch.no_grad():\n",
    "    for batch_src_ids, batch_tgt_input, batch_tgt_output in Hn_loader:\n",
    "        \n",
    "            batch_src_ids = batch_src_ids.to(device)\n",
    "            batch_tgt_input = batch_tgt_input.to(device)\n",
    "            batch_tgt_output = batch_tgt_output.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = Champion(batch_src_ids, batch_tgt_input)\n",
    "            \n",
    "            # flatten for loss to be (batch * seq_length, vocab_size)\n",
    "            \n",
    "            output = output.reshape(-1, output.size(-1))\n",
    "            \n",
    "            batch_tgt_output = batch_tgt_output.view(-1)\n",
    "            \n",
    "            \n",
    "            loss = Loss_Func(output, batch_tgt_output)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    scheduler.step()\n",
    "        \n",
    "    avg_loss = total_loss / len(Hn_loader)\n",
    "    print(f\"Epoch {epoch+1} | AVG Loss: {avg_loss:.4f}| Loss: {total_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
