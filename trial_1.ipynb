{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4b3ff1",
   "metadata": {},
   "source": [
    "# Topic 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e59d0",
   "metadata": {},
   "source": [
    "**Positional Encoding** This is used as in LLM they do not have recurrence as in sequential models they do not know how to treat sequence so for getting the position of tokens and providing sequence.\n",
    "\n",
    "Positional encoding is used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc6736b",
   "metadata": {},
   "source": [
    "## *Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f80a9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import sentencepiece as spm\n",
    "from datasets import Dataset\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2e09396",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings( category = FutureWarning , action =  'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc9c2a",
   "metadata": {},
   "source": [
    "##### Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ccecaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device : {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502ec41",
   "metadata": {},
   "source": [
    "## *Pos Encoding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d784401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positonal_encoding(seq_length , d_model):\n",
    "    \n",
    "    pe = torch.zeros(seq_length , d_model)\n",
    "    postions = torch.arange(0 , seq_length , dtype = torch.float32).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0 , d_model , 2).float() * (-math.log(10_000) / d_model))\n",
    "    \n",
    "    pe[: , 0::2] = torch.sin(postions * div_term)\n",
    "    pe[: , 1::2] = torch.cos(postions * div_term)\n",
    "    \n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda1891",
   "metadata": {},
   "source": [
    "# Attention Mechanism "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d91dc",
   "metadata": {},
   "source": [
    "The core of transformer is **Attention** we will start with *scaled dot product attention* \n",
    "\n",
    "1. In this a position token give attention to other tokens\n",
    "\n",
    "2. It helps to weighs the importance in the embedding of each token\n",
    "\n",
    "3. Softmax converts these weights to probabilites\n",
    "\n",
    "4. The dot product helps to see similarity between the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164b37bb",
   "metadata": {},
   "source": [
    "## *Scaled Dot product Attention*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d4b3be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " tensor([[[0.5216, 0.7006, 0.6354, 0.5056, 0.5217, 0.5956, 0.4607, 0.4217],\n",
      "         [0.5254, 0.6957, 0.6442, 0.5142, 0.5234, 0.5924, 0.4686, 0.4219],\n",
      "         [0.5253, 0.7074, 0.6406, 0.5035, 0.5170, 0.5959, 0.4565, 0.4170],\n",
      "         [0.5337, 0.6973, 0.6407, 0.5128, 0.5122, 0.5892, 0.4868, 0.4318]]])\n",
      "Attention Weights:\n",
      " tensor([[[0.2235, 0.3092, 0.2360, 0.2313],\n",
      "         [0.2337, 0.2857, 0.2515, 0.2290],\n",
      "         [0.2332, 0.3043, 0.2448, 0.2177],\n",
      "         [0.2521, 0.2797, 0.2352, 0.2331]]])\n"
     ]
    }
   ],
   "source": [
    "def scaled_dot_product_attention(Q , K , V):\n",
    "    \n",
    "    # Query is a 3D (batch , seq_length , d_k)\n",
    "    \n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Matmul is matrix multiplication\n",
    "    \n",
    "    scores = torch.matmul(Q , K.transpose(-2 , -1) / torch.sqrt(torch.tensor(d_k , dtype = torch.float32)))\n",
    "    \n",
    "    # K also has same dimension but the transpose changes its dimension to (batch , d_k , seq)\n",
    "    \n",
    "    attn_weights = torch.softmax(scores , dim = -1)\n",
    "    \n",
    "    # softmax convert the weights to probabilities\n",
    "    \n",
    "    output = torch.matmul(attn_weights , V)\n",
    "    \n",
    "    return output , attn_weights\n",
    "\n",
    "\n",
    "\n",
    "# Example input (batch size = 1, seq_len = 4, d_k = 8)\n",
    "Q = torch.rand(1, 4, 8)\n",
    "K = torch.rand(1, 4, 8)\n",
    "V = torch.rand(1, 4, 8)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(\"Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attn_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86fca4",
   "metadata": {},
   "source": [
    "# *Multi Head Attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8025bec",
   "metadata": {},
   "source": [
    "In this technique parallel layers will focus attention on dimension and is far more significantly impactful then single dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4911049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(Multi_Head_Attention , self).__init__()\n",
    "    \n",
    "        # Assert if num heads are applicable\n",
    "        try :\n",
    "            assert d_model % num_heads == 0\n",
    "        except:\n",
    "            print(f'Number of heads are not applicable on the dimension of model')\n",
    "            \n",
    "            \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        \n",
    "        # Create linear layers for Q , K , V\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        \n",
    "        self.projection = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def single_dot_attention(self, Q, K, V):\n",
    "        \n",
    "        d_k = Q.size(-1)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype = torch.float32))\n",
    "        \n",
    "        attention_weights = torch.softmax(scores, dim = -1)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projection\n",
    "        \n",
    "        Q = self.W_Q(query)\n",
    "        K = self.W_K(key)\n",
    "        V = self.W_V(value)\n",
    "        \n",
    "        def reshape(x):\n",
    "            x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            \n",
    "            return x\n",
    "\n",
    "        Q = reshape(Q)\n",
    "        K = reshape(K)\n",
    "        V = reshape(V)\n",
    "        \n",
    "        # Now add attention\n",
    "        \n",
    "        output , attn = self.single_dot_attention(Q, K, V)\n",
    "        \n",
    "        output = output.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        projection_out = self.projection(output)\n",
    "        \n",
    "        return projection_out , attn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ba87dc",
   "metadata": {},
   "source": [
    "# *Encoded Layer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af4c18",
   "metadata": {},
   "source": [
    "It intact all the layers:\n",
    "\n",
    "1.  Optinal(Layer Normalization)\n",
    "\n",
    "1. MHA\n",
    "\n",
    "2. Layer Normalization\n",
    "\n",
    "3. Feed forward Network\n",
    "\n",
    "4. Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d6cd1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        super(Transformer_Encoder, self).__init__()\n",
    "        \n",
    "        # Normalization\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        \n",
    "        # MHA\n",
    "        \n",
    "        self.MHA = Multi_Head_Attention(d_model, num_heads)\n",
    "        \n",
    "        # Layer Normalization 2\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Feed forward Network\n",
    "        \n",
    "        self.FFN = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_ff, d_ff),\n",
    "            \n",
    "            \n",
    "            nn.Linear(d_ff, d_ff),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer Normalization 3\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout layer\n",
    "        \n",
    "        self.drop_1 = nn.Dropout(dropout)\n",
    "        self.drop_2 = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Pass to norm 1\n",
    "        \n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Pass to MHA\n",
    "        \n",
    "        attn_output, _ = self.MHA(x, x, x)\n",
    "        \n",
    "        # Pass to norm 2\n",
    "        \n",
    "        x = self.norm2(x + self.drop_1(attn_output))\n",
    "        \n",
    "        # Pass to FFN\n",
    "        \n",
    "        ffn = self.FFN(x)\n",
    "        \n",
    "        # Pass to norm 3\n",
    "        \n",
    "        x = self.norm3(x + self.drop_2(ffn))\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e9d3e",
   "metadata": {},
   "source": [
    "# *Stack*\n",
    "\n",
    "Here we will stack the encoded layer for parallel and significant processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34c606fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Stack(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, num_heads, num_layer):\n",
    "        super(Transformer_Stack, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.layer = nn.ModuleList([Transformer_Encoder(d_model, num_heads, d_ff)\n",
    "                                    for _ in range(num_layer)])\n",
    "        \n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        pe = positonal_encoding(seq_length, self.d_model).unsqueeze(0).to(x.device)\n",
    "        \n",
    "        x = x + pe\n",
    "        \n",
    "        for layer in self.layer:\n",
    "            x = layer(x)\n",
    "            \n",
    "            \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c56db8",
   "metadata": {},
   "source": [
    "### Initailizating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28845d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder_stack = Transformer_Stack(\n",
    "    num_layer=6,                        \n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048\n",
    ")\n",
    "\n",
    "dummy_input = torch.rand(32, 50, 512)  # (batch_size, seq_len, d_model)\n",
    "out = encoder_stack(dummy_input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951b176",
   "metadata": {},
   "source": [
    "# *Transformer Decoder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "453f6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Decoder(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_heads, dropout = 0.1):\n",
    "        super(Transformer_Decoder, self).__init__()\n",
    "        \n",
    "        # Masked Self attention \n",
    "        \n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.masked = Multi_Head_Attention(d_model, num_heads)\n",
    "        \n",
    "        # Encoder Decoder Attention\n",
    "        \n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        self.enc_dec = Multi_Head_Attention(d_model, num_heads)\n",
    "        \n",
    "        # Feed Forward Network\n",
    "        \n",
    "        self.norm_3 = nn.LayerNorm(d_model)\n",
    "        self.FFN = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(d_ff, d_ff),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Norm\n",
    "        \n",
    "        self.norm_4 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropouts\n",
    "        \n",
    "        self.drop_1 = nn.Dropout(dropout)\n",
    "        self.drop_2 = nn.Dropout(dropout)\n",
    "        self.drop_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, enc_out, mask = None):\n",
    "        \n",
    "        # LayerNorm -> Masked \n",
    "        \n",
    "        x_2 = self.norm_1(x)\n",
    "        \n",
    "        attn_output, _ = self.masked(x_2, x_2, x_2 , mask)\n",
    "        \n",
    "        # Masked -> Drop -> LayerNorm -> Enc-Dec\n",
    "        \n",
    "        x = x + self.drop_1(attn_output)\n",
    "        \n",
    "        x_2 = self.norm_2(x)\n",
    "        \n",
    "        attn_output_2, _ = self.enc_dec(x_2, enc_out, enc_out)\n",
    "        \n",
    "        # Enc-Dec -> Drop -> LayerNorm -> FFN\n",
    "        \n",
    "        x = x + self.drop_2(attn_output_2)\n",
    "        \n",
    "        x_2 = self.norm_3(x)\n",
    "        \n",
    "        ffn_out = self.FFN(x_2)\n",
    "        \n",
    "        x_2 = x + self.drop_3(ffn_out)\n",
    "        \n",
    "        x = self.norm_4(x_2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5bb3ef",
   "metadata": {},
   "source": [
    "# *Masking*\n",
    "\n",
    "It prevents the transformer to see the future tokens and help it to learn to adjust what is has produced till now\n",
    "without it the transformer will never learn thus large losses and increased training time can be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "79106f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(seq_length):\n",
    "    \n",
    "    mask = torch.triu(torch.ones(seq_length, seq_length) , diagonal = 1)\n",
    "    \n",
    "    mask = mask.masked_fill(mask == 1 , -torch.inf)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f336e",
   "metadata": {},
   "source": [
    "Stacking the Decoding Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506bd18",
   "metadata": {},
   "source": [
    "# *Encoder Decoder Stack*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c096aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Decoder_Stack(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, num_heads, num_layer):\n",
    "        super(Encoder_Decoder_Stack, self).__init__()\n",
    "        \n",
    "        self.layer = nn.ModuleList([Transformer_Decoder(d_model, d_ff, num_heads)\n",
    "                                    for _ in range(num_layer)])\n",
    "        \n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x, enc_out):\n",
    "        \n",
    "        seq_length = x.size(0)\n",
    "        \n",
    "        pe = positonal_encoding(seq_length, self.d_model).to(x.device)\n",
    "        \n",
    "        x = x + pe\n",
    "        \n",
    "        mask = generate_mask(seq_length).to(x.device)\n",
    "        \n",
    "        for layer in self.layer:\n",
    "            x = layer(x, enc_out, mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b1e7c",
   "metadata": {},
   "source": [
    "### *Initialize*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c694a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "decoder_stack = Encoder_Decoder_Stack(\n",
    "    num_layer=6,     \n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048\n",
    ")\n",
    "\n",
    "dummy_input = torch.rand(32, 50, 512)  # (batch_size, seq_len, d_model)\n",
    "out = encoder_stack(dummy_input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56a2e4",
   "metadata": {},
   "source": [
    "# *Full Transformer*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23f46db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_ff, d_model, num_heads, num_layers, dropout = 0.1):\n",
    "        super(Full_Transformer, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings\n",
    "        \n",
    "        self.src_embedd = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedd = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Encoder Decoder Stack\n",
    "        \n",
    "        self.encoder = Transformer_Stack(d_model, d_ff, num_heads, num_layers)\n",
    "        self.decoder = Encoder_Decoder_Stack(d_model, d_ff, num_heads, num_layers)\n",
    "        \n",
    "        # Final Projection\n",
    "        \n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Dropout\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        \n",
    "        src_seq_length = src.size(0)\n",
    "        tgt_seq_length = tgt.size(0)\n",
    "        \n",
    "        # Embeddings\n",
    "        \n",
    "        src = self.src_embedd(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.tgt_embedd(tgt) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # Pos Encoding\n",
    "        \n",
    "        src_pe = positonal_encoding(src_seq_length, self.d_model).unsqueeze(0).to(src.device)\n",
    "        tgt_pe = positonal_encoding(tgt_seq_length, self.d_model).unsqueeze(0).to(tgt.device)\n",
    "        \n",
    "        # Drops\n",
    "        \n",
    "        src = self.drop(src + src_pe)\n",
    "        tgt = self.drop(tgt + tgt_pe)\n",
    "        \n",
    "        # Encoder pass\n",
    "        \n",
    "        enc_out = self.encoder(src)\n",
    "        \n",
    "        # Decoder pass\n",
    "        \n",
    "        dec_out = self.decoder(tgt, enc_out)\n",
    "        \n",
    "        # Final Projection\n",
    "        \n",
    "        out = self.projection(dec_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6707341",
   "metadata": {},
   "source": [
    "# *Tokenizer*\n",
    "\n",
    "Transformers works on numeric ids then words which leads to need of tokenizer.\n",
    "The **Tokenizer** used in this transformer is **Sentence Piece** and not an scratch cause it was not possible for me to train such large dataset on a manual build tokenizer i would eventually run of **memory and compututaion**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "61ef9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input = 'C:\\LLM & Agents\\HN.json',\n",
    "    model_prefix = 'HN_Tokenizer',\n",
    "    vocab_size = 800\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "519aa744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 235, 505, 58, 90, 28, 142, 114, 219, 4, 18, 4, 15, 288, 219, 4, 743, 89, 744, 143, 12, 4, 59, 26, 78, 54]\n",
      "What is the first law and most important principle\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "\n",
    "# Loading the tokenizer\n",
    "sp.Load('HN_Tokenizer.model')\n",
    "\n",
    "tokens = sp.Encode('What is the first law and most important principle' , int)\n",
    "print(tokens)\n",
    "\n",
    "decode = sp.decode(tokens)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f943fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = tgt_vocab_size = 800\n",
    "\n",
    "Champion = Full_Transformer(\n",
    "    src_vocab_size,\n",
    "    tgt_vocab_size,\n",
    "    d_ff = 2048,\n",
    "    d_model = 512,\n",
    "    num_heads = 8,\n",
    "    num_layers = 6\n",
    ").to(device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
